---
description: Test-Driven Development (TDD) patterns and testing standards
globs: "**/test_*.py", "**/tests/**/*.py", "pytest.ini", "conftest.py"
alwaysApply: true
---

# Test-Driven Development (TDD) & Testing Standards

## TDD Workflow Requirements
All feature development MUST follow the Red → Green → Refactor cycle:

1. **Red Phase**: Write failing tests that define expected behavior
2. **Green Phase**: Write minimal code to make tests pass
3. **Refactor Phase**: Improve code quality while maintaining test coverage

## Testing Infrastructure Architecture

### Test Structure Standards (Optimized - 7 Files)
```
jobs/tests/
├── conftest.py                      # Shared fixtures and pytest configuration
├── factories.py                     # Test data factories using factory_boy
├── mocks.py                        # Mock classes for external dependencies
├── test_setup_and_infrastructure.py # Django setup + testing infrastructure (consolidated)
├── test_models.py                  # Django model unit tests
├── test_views.py                   # API endpoint tests (DRF) - performance tests moved
├── test_serializers.py             # DRF serializer validation tests
├── test_tasks.py                   # Celery task tests
├── test_gpt_integration.py         # GPT client functionality tests
└── test_integration.py             # End-to-end integration + performance + GPT integration tests
```

### Consolidation Results (TASK-014 Completed)
- **Reduced from 10 files to 7 files** (-30% file reduction)
- **4,520+ lines → 4,364 lines** with enhanced functionality
- **88% test coverage maintained** (exceeds 70% target)
- **Enhanced performance testing**: load tests, capacity limits, memory profiling

### Test Configuration Standards

#### pytest.ini Configuration
```ini
[tool:pytest]
DJANGO_SETTINGS_MODULE = config.test_settings
python_files = tests.py test_*.py *_tests.py
python_classes = Test*
python_functions = test_*
testpaths = guideline_ingestion
pythonpath = . guideline_ingestion
django_find_project = false
addopts = 
    --verbose
    --tb=short
    --strict-markers
    --strict-config
    --cov=guideline_ingestion
    --cov-report=html:htmlcov
    --cov-report=term-missing
    --cov-report=xml
    --cov-config=.coveragerc
    --cov-fail-under=70
    --reuse-db
    --nomigrations
    --maxfail=3
    --durations=10
```

#### Test Settings (config/test_settings.py)
```python
from .settings import *

# Override environment for testing
ENVIRONMENT = 'testing'
IS_TESTING = True

# Test database configuration
DATABASES['default']['TEST'] = {'NAME': 'test_guideline_ingestion'}

# Celery eager mode for synchronous testing
CELERY_TASK_ALWAYS_EAGER = True
CELERY_TASK_EAGER_PROPAGATES = True
CELERY_BROKER_URL = 'memory://'

# Mock external services
OPENAI_API_KEY = 'test-api-key'

# Fast password hashing for tests
PASSWORD_HASHERS = ['django.contrib.auth.hashers.MD5PasswordHasher']

# Disable logging during tests
LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'handlers': {'null': {'class': 'logging.NullHandler'}},
    'root': {'handlers': ['null']},
}
```

## Test Categories and Markers

### Test Markers
```python
# Use pytest markers to categorize tests
@pytest.mark.unit          # Fast, isolated unit tests
@pytest.mark.integration   # Tests with database/external services
@pytest.mark.performance   # Performance and timing tests
@pytest.mark.api           # API endpoint tests
@pytest.mark.celery        # Celery task tests
@pytest.mark.openai        # Tests requiring OpenAI mocking
@pytest.mark.slow          # Slow tests (>1 second)
```

### Running Specific Test Categories
```bash
# Unit tests only (fast)
docker compose exec app /app/test-docker.sh -m unit

# Integration tests
docker compose exec app /app/test-docker.sh -m integration

# Performance tests
docker compose exec app /app/test-docker.sh -m performance

# Exclude slow tests
docker compose exec app /app/test-docker.sh -m "not slow"
```

## Mock Infrastructure Patterns

### External Service Mocking

#### OpenAI API Mocking
```python
class MockOpenAIClient:
    """Mock OpenAI client for consistent testing."""
    
    def __init__(self):
        self.call_count = 0
        self.call_history = []
    
    def create_chat_completion(self, messages, model="gpt-3.5-turbo", **kwargs):
        self.call_count += 1
        self.call_history.append({'messages': messages, 'model': model})
        
        if 'summarize' in str(messages).lower():
            return self._create_summary_response()
        return self._create_checklist_response()

# Context manager for easy mocking
@contextmanager
def mock_openai_client():
    with patch('openai.OpenAI') as mock:
        mock_client = MockOpenAIClient()
        mock.return_value.chat.completions.create = mock_client.create_chat_completion
        yield mock_client
```

#### Redis and Celery Mocking
```python
class MockRedisClient:
    def __init__(self):
        self.data = {}
        self.call_count = 0
    
    def ping(self): return True
    def set(self, key, value, ex=None): 
        self.data[key] = value
        return True
    def get(self, key): 
        return self.data.get(key)

@contextmanager
def mock_celery_task(task_id=None, result=None):
    mock_task = MockCeleryTask(task_id, result)
    with patch('jobs.tasks.process_job') as mock_process:
        mock_process.delay = mock_task.delay
        yield mock_task
```

## Test Data Factory Patterns

### Using factory_boy for Test Data
```python
import factory
from datetime import datetime, timezone

class JobDataFactory(factory.Factory):
    """Factory for creating job data dictionaries."""
    
    class Meta:
        model = dict
    
    guidelines = factory.Faker('text', max_nb_chars=1000)
    title = factory.Faker('sentence', nb_words=3)
    description = factory.Faker('text', max_nb_chars=200)

# Model factory (when Job model is implemented)
class JobFactory(factory.django.DjangoModelFactory):
    class Meta:
        model = 'jobs.Job'
    
    event_id = factory.LazyFunction(lambda: str(uuid.uuid4()))
    status = factory.Iterator(['PENDING', 'PROCESSING', 'COMPLETED'])
    input_data = factory.SubFactory(JobDataFactory)
    created_at = factory.LazyFunction(lambda: datetime.now(timezone.utc))
```

## Fixture Patterns

### Common Fixtures (conftest.py)
```python
@pytest.fixture
def api_client():
    """DRF API client for testing endpoints."""
    from rest_framework.test import APIClient
    return APIClient()

@pytest.fixture
def sample_job_data():
    """Sample job data for testing."""
    return JobDataFactory()

@pytest.fixture
def performance_timer():
    """Timer for performance testing."""
    class Timer:
        def start(self): self.start_time = time.time()
        def stop(self): self.end_time = time.time()
        @property
        def elapsed_ms(self): return (self.end_time - self.start_time) * 1000
    return Timer()

@pytest.fixture
def mock_openai_responses():
    """Mock OpenAI API responses."""
    return {
        'summary': {
            'choices': [{'message': {'content': 'Summary: Test guidelines.'}}]
        },
        'checklist': {
            'choices': [{'message': {'content': 'Checklist:\n□ Item 1\n□ Item 2'}}]
        }
    }
```

## TDD Test Patterns

### Django Setup Testing Pattern
```python
class TestDjangoConfiguration:
    """Test Django project configuration and setup."""
    
    @pytest.mark.unit
    def test_settings_module_loads(self):
        """Test Django settings module loads without errors."""
        from django.conf import settings
        assert settings.configured
        assert hasattr(settings, 'SECRET_KEY')
        assert hasattr(settings, 'DATABASES')
    
    @pytest.mark.unit
    def test_required_apps_installed(self):
        """Test all required Django apps are installed."""
        from django.conf import settings
        required_apps = [
            'django.contrib.admin',
            'rest_framework',
            'drf_spectacular',
            'jobs',
        ]
        for app in required_apps:
            assert app in settings.INSTALLED_APPS
    
    @pytest.mark.integration
    def test_database_connectivity(self, db):
        """Test database connection works in container."""
        from django.db import connection
        with connection.cursor() as cursor:
            cursor.execute("SELECT 1")
            result = cursor.fetchone()
            assert result[0] == 1
    
    @pytest.mark.unit
    def test_environment_aware_configuration(self):
        """Test configuration adapts to environment."""
        from django.conf import settings
        assert hasattr(settings, 'ENVIRONMENT')
        assert hasattr(settings, 'IS_TESTING')
        
        # Tests should run in testing environment
        if settings.IS_TESTING:
            assert settings.CELERY_TASK_ALWAYS_EAGER is True
```

### Model Testing Pattern
```python
class TestJobModel:
    """Test Job model functionality."""
    
    def test_job_creation_with_required_fields(self, db):
        """Test job creation with minimum required fields."""
        job_data = {'guidelines': 'Test guidelines'}
        job = Job.objects.create(input_data=job_data)
        
        assert job.event_id is not None
        assert job.status == JobStatus.PENDING
        assert job.input_data == job_data
        assert job.created_at is not None
    
    def test_job_status_transitions(self, db):
        """Test valid job status transitions."""
        job = Job.objects.create(input_data={'guidelines': 'test'})
        
        # PENDING → PROCESSING
        job.status = JobStatus.PROCESSING
        job.save()
        assert job.status == JobStatus.PROCESSING
        
        # PROCESSING → COMPLETED
        job.status = JobStatus.COMPLETED
        job.save()
        assert job.status == JobStatus.COMPLETED
```

### API Testing Pattern
```python
class TestJobAPI:
    """Test job API endpoints."""
    
    @pytest.mark.api
    def test_create_job_success(self, api_client, sample_job_data):
        """Test successful job creation."""
        response = api_client.post('/jobs/', sample_job_data)
        
        assert response.status_code == 201
        assert 'event_id' in response.data
        assert response.data['status'] == 'PENDING'
    
    @pytest.mark.api
    def test_create_job_validation_error(self, api_client):
        """Test job creation with invalid data."""
        invalid_data = {'guidelines': ''}  # Too short
        response = api_client.post('/jobs/', invalid_data)
        
        assert response.status_code == 400
        assert 'error' in response.data
        assert response.data['error']['code'] == 'VALIDATION_ERROR'
```

### Performance Testing Pattern (Enhanced in TASK-014)
```python
class TestPerformanceValidation(APITestCase):
    """Consolidated performance testing (in test_integration.py)."""
    
    @pytest.mark.performance
    def test_job_create_response_time_performance(self):
        """Test job creation meets <200ms requirement."""
        start_time = time.time()
        response = self.client.post(self.create_url, data=json.dumps(self.valid_payload))
        response_time_ms = (time.time() - start_time) * 1000
        
        assert response.status_code == 201
        assert response_time_ms < 200, f"Response time {response_time_ms}ms exceeds 200ms"
    
    @pytest.mark.performance
    def test_concurrent_request_performance(self):
        """Test API performance with 10 concurrent requests."""
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(self.create_job) for _ in range(10)]
            results = [future.result() for future in concurrent.futures.as_completed(futures)]
        
        for status_code, response_time in results:
            assert status_code == 201
            assert response_time < 200, f"Response time {response_time}ms exceeds 200ms"
    
    @pytest.mark.performance
    def test_large_dataset_query_performance(self):
        """Test database query performance with 100+ jobs."""
        # Create 100 jobs for performance testing
        jobs = [Job.objects.create(...) for i in range(100)]
        
        start_time = time.time()
        all_jobs = Job.objects.all().count()
        query_time = (time.time() - start_time) * 1000
        
        assert query_time < 100, f"Query time {query_time}ms exceeds 100ms"
    
    @pytest.mark.performance
    def test_system_behavior_at_capacity_limits(self):
        """Test system behavior with 200+ jobs (capacity limits)."""
        bulk_jobs = [Job(...) for i in range(200)]
        
        start_time = time.time()
        Job.objects.bulk_create(bulk_jobs)
        creation_time = (time.time() - start_time) * 1000
        
        assert creation_time < 5000, f"Bulk creation {creation_time}ms exceeds 5s"
    
    @pytest.mark.performance
    def test_queue_performance_under_high_load(self):
        """Test queue performance with 50 concurrent job submissions."""
        with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:
            futures = [executor.submit(self.submit_job, i) for i in range(50)]
            results = [future.result() for future in concurrent.futures.as_completed(futures)]
        
        success_rate = sum(1 for r in results if r[0] == 'success') / len(results)
        assert success_rate > 0.95, f"Success rate {success_rate:.2%} below 95%"
    
    @pytest.mark.performance
    def test_memory_usage_during_bulk_operations(self):
        """Test memory usage during bulk operations with 50 jobs."""
        import psutil, os
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # Perform bulk operations
        bulk_jobs = [Job(...) for i in range(50)]
        Job.objects.bulk_create(bulk_jobs)
        
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory
        
        assert memory_increase < 50, f"Memory increase {memory_increase}MB exceeds 50MB"
```

### Celery Task Testing Pattern
```python
class TestCeleryTasks:
    """Test Celery task functionality."""
    
    @pytest.mark.celery
    def test_process_job_task_with_mock_openai(self, db, mock_openai_responses):
        """Test job processing task with mocked OpenAI."""
        with mock_openai_client() as mock_client:
            job = Job.objects.create(input_data={'guidelines': 'test'})
            
            # Run task (synchronous in test mode)
            result = process_job.delay(job.id, 'test guidelines')
            
            # Verify task completed
            assert result.status == 'SUCCESS'
            assert mock_client.call_count == 2  # Summary + checklist calls
            
            # Verify job was updated
            job.refresh_from_db()
            assert job.status == JobStatus.COMPLETED
            assert job.result is not None
```

## Coverage Requirements

### Coverage Targets
- **Overall Coverage**: ≥70%
- **Model Coverage**: ≥90%
- **View Coverage**: ≥85%
- **Task Coverage**: ≥80%

### Coverage Configuration (.coveragerc)
```ini
[run]
source = guideline_ingestion
omit = 
    */migrations/*
    */tests/*
    */venv/*
    manage.py
    */settings/*
branch = True

[report]
exclude_lines =
    pragma: no cover
    def __repr__
    raise AssertionError
    raise NotImplementedError
show_missing = True
```

### Coverage Reporting
```bash
# Generate HTML coverage report
docker compose exec app /app/test-docker.sh --cov-report=html

# View coverage in browser
open htmlcov/index.html

# Terminal coverage report
docker compose exec app /app/test-docker.sh --cov-report=term-missing
```

## Integration Testing Standards

### Database Integration
```python
@pytest.mark.integration
def test_job_creation_with_database(self, db):
    """Test job creation integrates properly with database."""
    job_data = {'guidelines': 'Integration test guidelines'}
    job = Job.objects.create(input_data=job_data)
    
    # Verify database persistence
    retrieved_job = Job.objects.get(event_id=job.event_id)
    assert retrieved_job.input_data == job_data
```

### End-to-End Testing
```python
@pytest.mark.integration
def test_complete_job_processing_flow(self, api_client, sample_job_data):
    """Test complete job processing from API to completion."""
    with mock_openai_client():
        # 1. Create job via API
        response = api_client.post('/jobs/', sample_job_data)
        assert response.status_code == 201
        event_id = response.data['event_id']
        
        # 2. Job should be processed (eager mode in tests)
        # 3. Check job status
        status_response = api_client.get(f'/jobs/{event_id}/')
        assert status_response.status_code == 200
        assert status_response.data['status'] == 'COMPLETED'
```

## Test Execution Commands

### Local Development Testing
```bash
# Run all tests with coverage
docker compose exec app /app/test-docker.sh

# Run specific test file
docker compose exec app /app/test-docker.sh jobs/tests/test_models.py

# Run with specific markers
docker compose exec app /app/test-docker.sh -m "unit and not slow"

# Run with coverage and stop on first failure
docker compose exec app /app/test-docker.sh --maxfail=1 --cov-report=term
```

### CI/CD Pipeline Testing
```bash
# Full test suite with coverage requirements
docker compose exec app /app/test-docker.sh --cov-fail-under=70

# Performance test validation
docker compose exec app /app/test-docker.sh -m performance --tb=short

# Integration test validation
docker compose exec app /app/test-docker.sh -m integration --maxfail=5
```